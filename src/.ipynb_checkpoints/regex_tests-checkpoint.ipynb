{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections(text):\n",
    "     return re.findall(r\"\\\\(?:sub)*section{.*}\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figure(text):\n",
    "    return re.findall(r'((?<!\\\\begin{comment})\\s\\\\begin{(?P<sec>(?:figure\\*{0,1}|itemize|equation|table))}(?:.*?)\\\\end{(?P=sec)})', text, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(text):\n",
    "    return re.findall(r'(\\\\(?:sub)*section{.*?})|((?<!\\\\begin{comment})\\s\\\\begin{(?P<sec>(?:figure\\*{0,1}|itemize|equation|table))}(?:.*?)\\\\end{(?P=sec)})', text, re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = open(\"/home/jessica/Python_Code/PaperHelper/test/resources/ex1_original.tex\", 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\section{Introduction}', '\\\\section{Related Work}', '\\\\subsection{Aspect Level Sentiment Classification}', '\\\\subsection{Multi-task Learning}', '\\\\section{Model}', '\\\\subsection{Input and Embedding Layers}', '\\\\subsection{LSTM Layer}', '\\\\subsection{Task-Specific Attention Layer}', '\\\\subsection{Regularization Layer}', '\\\\section{Experiments}', '\\\\subsection{Datasets}', '\\\\subsection{Comparison Methods}', '\\\\subsection{Results}', '\\\\section{Conclusion}']\n"
     ]
    }
   ],
   "source": [
    "res = get_sections(test1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "('\\\\section{Introduction}', '', '')\n",
      "('\\\\section{Introduction}', '', '')\n",
      "('\\\\section{Introduction}', '', '')\n"
     ]
    }
   ],
   "source": [
    "r = get_all(test1)[0]\n",
    "print(len(r))\n",
    "for e in r:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "\n",
      "\\begin{figure}\n",
      "\\setlength{\\abovecaptionskip}{0.2cm}    %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离\n",
      "\\centering\n",
      "\\includegraphics[width=0.45\\textwidth]{example.pdf}\n",
      "\\caption{Example of a non-overlapping sentence.} \n",
      "  \\label{sentence} \n",
      "\\end{figure}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{itemize}\n",
      "\\vspace{-0.2cm}\n",
      "\\item We propose CAN for multi-aspect sentiment analysis. Specifically, we introduce {\\bf orthogonal} and {\\bf sparse} regularizations to constrain the attention weight allocation, helping learn better aspect-specific sentence representations. To the best of our knowledge, this is the first work for multi-aspect sentiment analysis.\n",
      "\n",
      "%\\vspace{-0.2cm}\n",
      "%\\item \\textcolor{red}{For accurately constraining the sentences, we annotate two public datasets about whether multiple aspects in the sentence are overlap, and we will publish the datasets.} \n",
      "\n",
      "\\vspace{-0.2cm}\n",
      "\\item We extend CAN to multi-task settings by introducing ACD as an auxiliary task, and applying CAN on  both ALSC and ACD tasks. \n",
      "\n",
      "\\vspace{-0.2cm}\n",
      "\\item Extensive experiments are conducted on public datasets. Results demonstrate the effectiveness of our approach for aspect level sentiment classification.  \n",
      "\\end{itemize}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{figure*}\n",
      "\\setlength{\\abovecaptionskip}{0.2cm}   %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.3cm}   %调整图片标题与下文距离\n",
      "\\centering\n",
      "\t\\includegraphics[width=1.0\\textwidth]{Big_network_new.pdf}\n",
      "    \\caption{Network Architecture. The aspect categories are embedded as vectors. The model encodes the sentence using LSTM. Based on its hidden states, aspect-specific sentence representations for ALSC and ACD tasks are learned via constrained attention. Then aspect level sentiment prediction and aspect category detection are made. }\n",
      "    \\label{network}\n",
      "\\end{figure*}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{equation} \n",
      "\\setlength{\\abovedisplayskip}{4pt}  % 公式与上文间距\n",
      "\\setlength{\\belowdisplayskip}{4pt}  % 公式与下文间距\n",
      "   h_l = LSTM(h_{l-1},v_l)\n",
      "\\end{equation}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{equation} \n",
      "\\setlength{\\abovedisplayskip}{4pt}  % 公式与上文间距\n",
      "\\setlength{\\belowdisplayskip}{4pt}  % 公式与下文间距\n",
      "    \\alpha_k = softmax({z^a}^\\mathrm{T}tanh(W_{1}^{a}{H} + W_{2}^{a}(u_k^{s}\\otimes{e_L}))) \n",
      "  \\label{equation_absa_att}\n",
      "\\end{equation}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{equation} \n",
      "\\setlength{\\abovedisplayskip}{4pt}  % 公式与上文间距\n",
      "\\setlength{\\belowdisplayskip}{4pt}  % 公式与下文间距\n",
      "    \\beta_n = softmax({z^b}^\\mathrm{T}tanh(W_{1}^{b}{H} + W_{2}^{b}(u_n\\otimes{e_L}))) \n",
      "  \\label{equation_acd_att}\n",
      "\\end{equation}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{equation} \n",
      "\\setlength{\\abovedisplayskip}{4pt}  % 公式与上文间距\n",
      "\\setlength{\\belowdisplayskip}{4pt}  % 公式与下文间距\n",
      "    R_s = \\mid\\sum\\limits_{l=1}^L{\\alpha_{kl}^{2}} - 1\\mid\n",
      "    \\label{equation:sparse_reg}\n",
      "\\end{equation}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "\\setlength{\\abovedisplayskip}{4pt}  % 公式与上文间距\n",
      "\\setlength{\\belowdisplayskip}{4pt}  % 公式与下文间距\n",
      "\tR_o = \\parallel{M^{\\mathrm{T}}}M - I\\parallel_2\n",
      "\\end{equation}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{table}[t!]\n",
      "\\setlength{\\abovecaptionskip}{0.1cm}   %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离\n",
      "\\begin{center}\n",
      "\\setlength{\\tabcolsep}{1mm}{\n",
      "\\begin{tabular} {|c|c|ccc|c|}\n",
      "\\hline\n",
      "    \\multirow{2}{*}{Dataset} & \\multirow{2}{*}{\\#Single} & \\multicolumn{3}{c|}{\\#Multi} & \\multirow{2}{*}{\\#Total} \\\\\n",
      "    \\cline{3-5}\n",
      "      &&  \\emph{OL} & \\emph{NOL} & \\emph{Total} & \\\\\n",
      "\t\\hline\n",
      "\t\tRest14\\_Train & 2053 & 67 & 415 & 482 & 2535\\\\\n",
      "        Rest14\\_Val & 412 & 19 & 75 & 94 & 506 \\\\\n",
      "\t\tRest14\\_Test & 611 & 27 & 162 & 189 & 800 \\\\\n",
      "        \\hline\n",
      "        Rest15\\_Train & 622 & 47 & 262 & 309 & 931 \\\\\n",
      "        Rest15\\_Val & 137 & 13 & 39 & 52 & 189 \\\\\n",
      "        Rest15\\_Test & 390 & 30 & 162 & 192 & 582 \\\\\n",
      "\t\\hline\n",
      "\\end{tabular}}\n",
      "\\end{center}\n",
      "\\caption{\\label{table-dataset} The numbers of single- and multi-aspect sentences. \\emph{OL} and \\emph{NOL} denote the overlapping and non-overlapping multi-aspect sentences, respectively.}\t\n",
      "\\end{table}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item {\\bf LSTM}: We implement the vanilla LSTM networks to model the sentence and use the average of all hidden states as the sentence representation. In this model, aspect information is not used.\n",
      "\n",
      "\\vspace{-6pt}\n",
      "\\item {\\bf AT-LSTM} \\cite{Wang2016Attention}: It adopts the attention mechanism in LSTM to generate a weighted representation of a sentence. The aspect embedding is used to compute the attention weights as in Equation \\ref{equation_absa_att}. We do not concatenate the aspect embedding to the hidden state as in \\cite{Wang2016Attention} and gain small performance improvement. We use this modified version of AT-LSTM in all experiments. \n",
      "\n",
      "\\vspace{-6pt}\n",
      "\\item {\\bf ATAE-LSTM} \\cite{Wang2016Attention}: This method is an extension of AT-LSTM. In this model, the aspect embedding is concatenated to each word embedding of the sentence as the input to the LSTM layer. \n",
      "\n",
      "%\\vspace{-5pt}\n",
      "%\\item {\\bf AF-LSTM(CONV)} \\cite{Tay2017Learning}: It utilizes circular convolution to compute deeper fusion relationships between each word in sentence and aspect.\n",
      "\\end{itemize}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{table}[t!]\n",
      "\\setlength{\\abovecaptionskip}{0.0cm}   %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离\n",
      "\\begin{center}\n",
      "\\setlength{\\tabcolsep}{1.5mm}{\n",
      "\\begin{tabular} {|c|cc|cc|}\n",
      "\\hline\n",
      "\t\\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{Rest14} & \\multicolumn{2}{c|}{Rest15} \\\\ \n",
      "    \\cline{2-5}\n",
      "    & 3-way & Binary & 3-way & Binary \\\\    \n",
      "\t\\hline\n",
      "    LSTM      & 80.61 & 86.66 & 73.14 & 73.27 \\\\ \n",
      "    AT-LSTM   & 81.66 & 87.13 & 75.15 & 76.40 \\\\\n",
      "    ATAE-LSTM & 82.08 & 87.72 & 74.32 & 76.79 \\\\\n",
      "    %AF-LSTM(CONV) & 81.29 & 87.26 & - & - \\\\\n",
      "    \\hline\n",
      "    AT-CAN-$R_s$ & 81.97 & 88.08 & 75.74 & 80.05 \\\\\n",
      "    AT-CAN-$R_o$ & 82.60 & 88.67 & 75.03 & 81.10 \\\\\n",
      "    ATAE-CAN-$R_s$ & 82.29 & 87.37 &  76.09 & 80.83 \\\\\n",
      "    ATAE-CAN-$R_o$ & {\\bf 82.91} & {\\bf 89.02} & {\\bf 77.28} & {\\bf 82.66} \\\\\n",
      "\t\\hline\n",
      "\\end{tabular}}\n",
      "\\end{center}\n",
      "\\caption{\\label{table-st} Results of the ALSC task in terms of accuracy ($\\%$). All methods are run in single-task settings.}\t\n",
      "\\end{table}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{figure}\n",
      "\\setlength{\\abovecaptionskip}{0.1cm}   %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离\n",
      "\\centering\n",
      "\\subfigure[AT-LSTM]{\n",
      "\\includegraphics[width=0.45\\textwidth]{at_.pdf}}\n",
      "\\vspace{-5pt}\n",
      "\\subfigure[M-AT-LSTM]{\n",
      "\\includegraphics[width=0.45\\textwidth]{at_r2_.pdf}}\n",
      "\\vspace{-5pt}\n",
      "\\subfigure[M-CAN-2$R_o$]{\n",
      "\\includegraphics[width=0.45\\textwidth]{multitask_at_2r2_.pdf}}\n",
      "\\caption{Visualization of attention weights of different aspects in the ALSC task. Three different models are compared.}\n",
      "\\label{compare-att}\n",
      "\\end{figure}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{figure}\n",
      "\\setlength{\\abovecaptionskip}{0.1cm}   %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离\n",
      "\\centering\n",
      "\\includegraphics[width=0.45\\textwidth]{f3_.pdf}\n",
      "\\caption{Visualization of attention weights of different aspects in the ACD task from M-CAN-2$R_o$. The a/m is short for anecdotes/miscellaneous.} \n",
      "  \\label{ACD-att} \n",
      "\\end{figure}\n",
      "\n",
      " ---------- \n",
      "\n",
      "\n",
      "\\begin{figure}\n",
      "\\setlength{\\abovecaptionskip}{0.1cm}   %调整图片标题与图距离\n",
      "\\setlength{\\belowcaptionskip}{-0.5cm}   %调整图片标题与下文距离\n",
      "\\centering\n",
      "\\includegraphics[width=0.45\\textwidth]{loss.pdf}\n",
      "\\caption{The regularization loss curves of $R_s$ and $R_o$ during the training of AT-CAN-$R_o$.} \n",
      "  \\label{figure:reg-loss} \n",
      "\\end{figure}\n",
      "\n",
      " ---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = get_figure(test1)\n",
    "print(len(res))\n",
    "for e in res:\n",
    "    print(e[0])\n",
    "    print(\"\\n ---------- \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
