\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{bm}
\usepackage{array}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{url}
\usepackage{color}

\usepackage{verbatim}


\newcommand\BibTeX{B{\sc ib}\TeX}

\title{CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis}
% \author{
%   Mengting Hu \\
%   {\tt mthu@mail.nankai.edu.cn} \\
%   {Nankai University, Tianjin, China} \\
%   \And
%   Shiwan Zhao \\
%   {\tt zhaosw@cn.ibm.com }      \\
%   {IBM Research - China, Beijing , China}
%   }
\author{Mengting Hu\textsuperscript{1}\thanks{\quad This work was done when Mengting Hu was a research intern at IBM Research - China.}, Shiwan Zhao\textsuperscript{2}, Li Zhang\textsuperscript{2}, Keke Cai\textsuperscript{2},
Zhong Su\textsuperscript{2}, Renhong Cheng\textsuperscript{1}, Xiaowei Shen\textsuperscript{2} \\
\textsuperscript{1} Nankai University, \textsuperscript{2} IBM Research - China \\
mthu@mail.nankai.edu.cn, \{zhaosw, lizhang, caikeke, suzhong\}@cn.ibm.com, \\ chengrh@nankai.edu.cn, xwshen@cn.ibm.com
}

\begin{document}
\maketitle
\begin{abstract}
Aspect level sentiment classification is a fine-grained sentiment analysis task, compared to the sentence level classification. A sentence usually contains one or more aspects. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various methods for generating aspect-specific sentence representations. However, these studies handle each aspect of a sentence separately. In this paper, we argue that multiple aspects of a sentence are usually orthogonal based on the observation that different aspects concentrate on different parts of the sentence. To force the orthogonality among aspects, we propose constrained attention networks (CAN) for multi-aspect sentiment analysis, which handles multiple aspects of a sentence simultaneously. Experimental results on two public datasets demonstrate the effectiveness of our approach. We also extend our approach to multi-task settings, outperforming the state-of-the-arts significantly.
\end{abstract}


\section{Introduction}


\begin{figure}
\setlength{\abovecaptionskip}{0.2cm}    %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=0.45\textwidth]{example.pdf}
\caption{Example of a non-overlapping sentence.} 
  \label{sentence} 
\end{figure}


\begin{itemize}
\vspace{-0.2cm}
\item We propose CAN for multi-aspect sentiment analysis. Specifically, we introduce {\bf orthogonal} and {\bf sparse} regularizations to constrain the attention weight allocation, helping learn better aspect-specific sentence representations. To the best of our knowledge, this is the first work for multi-aspect sentiment analysis.

%\vspace{-0.2cm}
%\item \textcolor{red}{For accurately constraining the sentences, we annotate two public datasets about whether multiple aspects in the sentence are overlap, and we will publish the datasets.} 

\vspace{-0.2cm}
\item We extend CAN to multi-task settings by introducing ACD as an auxiliary task, and applying CAN on  both ALSC and ACD tasks. 

\vspace{-0.2cm}
\item Extensive experiments are conducted on public datasets. Results demonstrate the effectiveness of our approach for aspect level sentiment classification.  
\end{itemize}

\section{Related Work}

\subsection{Aspect Level Sentiment Classification}

\subsection{Multi-task Learning}


\begin{figure*}
\setlength{\abovecaptionskip}{0.2cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.3cm}   %调整图片标题与下文距离
\centering
	\includegraphics[width=1.0\textwidth]{Big_network_new.pdf}
    \caption{Network Architecture. The aspect categories are embedded as vectors. The model encodes the sentence using LSTM. Based on its hidden states, aspect-specific sentence representations for ALSC and ACD tasks are learned via constrained attention. Then aspect level sentiment prediction and aspect category detection are made. }
    \label{network}
\end{figure*}

\section{Model}

\subsection{Input and Embedding Layers}

\subsection{LSTM Layer}


\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
   h_l = LSTM(h_{l-1},v_l)
\end{equation}

\subsection{Task-Specific Attention Layer}


\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
    \alpha_k = softmax({z^a}^\mathrm{T}tanh(W_{1}^{a}{H} + W_{2}^{a}(u_k^{s}\otimes{e_L}))) 
  \label{equation_absa_att}
\end{equation}


\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
    \beta_n = softmax({z^b}^\mathrm{T}tanh(W_{1}^{b}{H} + W_{2}^{b}(u_n\otimes{e_L}))) 
  \label{equation_acd_att}
\end{equation}

\subsection{Regularization Layer}


\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
    R_s = \mid\sum\limits_{l=1}^L{\alpha_{kl}^{2}} - 1\mid
    \label{equation:sparse_reg}
\end{equation}


\begin{equation}
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
	R_o = \parallel{M^{\mathrm{T}}}M - I\parallel_2
\end{equation}

\section{Experiments}

\subsection{Datasets}


\begin{table}[t!]
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离
\begin{center}
\setlength{\tabcolsep}{1mm}{
\begin{tabular} {|c|c|ccc|c|}
\hline
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{\#Single} & \multicolumn{3}{c|}{\#Multi} & \multirow{2}{*}{\#Total} \\
    \cline{3-5}
      &&  \emph{OL} & \emph{NOL} & \emph{Total} & \\
	\hline
		Rest14\_Train & 2053 & 67 & 415 & 482 & 2535\\
        Rest14\_Val & 412 & 19 & 75 & 94 & 506 \\
		Rest14\_Test & 611 & 27 & 162 & 189 & 800 \\
        \hline
        Rest15\_Train & 622 & 47 & 262 & 309 & 931 \\
        Rest15\_Val & 137 & 13 & 39 & 52 & 189 \\
        Rest15\_Test & 390 & 30 & 162 & 192 & 582 \\
	\hline
\end{tabular}}
\end{center}
\caption{\label{table-dataset} The numbers of single- and multi-aspect sentences. \emph{OL} and \emph{NOL} denote the overlapping and non-overlapping multi-aspect sentences, respectively.}	
\end{table}

\subsection{Comparison Methods}


\begin{itemize}
\item {\bf LSTM}: We implement the vanilla LSTM networks to model the sentence and use the average of all hidden states as the sentence representation. In this model, aspect information is not used.

\vspace{-6pt}
\item {\bf AT-LSTM} \cite{Wang2016Attention}: It adopts the attention mechanism in LSTM to generate a weighted representation of a sentence. The aspect embedding is used to compute the attention weights as in Equation \ref{equation_absa_att}. We do not concatenate the aspect embedding to the hidden state as in \cite{Wang2016Attention} and gain small performance improvement. We use this modified version of AT-LSTM in all experiments. 

\vspace{-6pt}
\item {\bf ATAE-LSTM} \cite{Wang2016Attention}: This method is an extension of AT-LSTM. In this model, the aspect embedding is concatenated to each word embedding of the sentence as the input to the LSTM layer. 

%\vspace{-5pt}
%\item {\bf AF-LSTM(CONV)} \cite{Tay2017Learning}: It utilizes circular convolution to compute deeper fusion relationships between each word in sentence and aspect.
\end{itemize}


\begin{table}[t!]
\setlength{\abovecaptionskip}{0.0cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离
\begin{center}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular} {|c|cc|cc|}
\hline
	\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Rest14} & \multicolumn{2}{c|}{Rest15} \\ 
    \cline{2-5}
    & 3-way & Binary & 3-way & Binary \\    
	\hline
    LSTM      & 80.61 & 86.66 & 73.14 & 73.27 \\ 
    AT-LSTM   & 81.66 & 87.13 & 75.15 & 76.40 \\
    ATAE-LSTM & 82.08 & 87.72 & 74.32 & 76.79 \\
    %AF-LSTM(CONV) & 81.29 & 87.26 & - & - \\
    \hline
    AT-CAN-$R_s$ & 81.97 & 88.08 & 75.74 & 80.05 \\
    AT-CAN-$R_o$ & 82.60 & 88.67 & 75.03 & 81.10 \\
    ATAE-CAN-$R_s$ & 82.29 & 87.37 &  76.09 & 80.83 \\
    ATAE-CAN-$R_o$ & {\bf 82.91} & {\bf 89.02} & {\bf 77.28} & {\bf 82.66} \\
	\hline
\end{tabular}}
\end{center}
\caption{\label{table-st} Results of the ALSC task in terms of accuracy ($\%$). All methods are run in single-task settings.}	
\end{table}

\subsection{Results}


\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离
\centering
\subfigure[AT-LSTM]{
\includegraphics[width=0.45\textwidth]{at_.pdf}}
\vspace{-5pt}
\subfigure[M-AT-LSTM]{
\includegraphics[width=0.45\textwidth]{at_r2_.pdf}}
\vspace{-5pt}
\subfigure[M-CAN-2$R_o$]{
\includegraphics[width=0.45\textwidth]{multitask_at_2r2_.pdf}}
\caption{Visualization of attention weights of different aspects in the ALSC task. Three different models are compared.}
\label{compare-att}
\end{figure}


\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=0.45\textwidth]{f3_.pdf}
\caption{Visualization of attention weights of different aspects in the ACD task from M-CAN-2$R_o$. The a/m is short for anecdotes/miscellaneous.} 
  \label{ACD-att} 
\end{figure}


\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.5cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=0.45\textwidth]{loss.pdf}
\caption{The regularization loss curves of $R_s$ and $R_o$ during the training of AT-CAN-$R_o$.} 
  \label{figure:reg-loss} 
\end{figure}

\section{Conclusion}

\end{document}