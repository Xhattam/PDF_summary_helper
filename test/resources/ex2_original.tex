\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2018}
%\hypersetup{draft}

\usepackage{times}
\usepackage{latexsym}
\usepackage{pbox}
\usepackage{url}
\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP 2018}
\newcommand\conforg{SIGDAT}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage{color}
\usepackage{graphicx}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}

\newtheoremstyle{mydef}%
{}{}{\normalfont}{}{\itshape}{.\,}{ }{}
\theoremstyle{mydef}
\newtheorem{definition}{Definition}

\newtheoremstyle{myprob}%
{}{}{\normalfont}{}{\scshape}{.\,}{ }{}
\theoremstyle{myprob}
\newtheorem{problem}{Problem}

\newtheoremstyle{mylayer}{}{}{\normalfont}{}{\itshape}{.\,}{ }{}\theoremstyle{mylayer}
\newtheorem{layer1}{}

%%%%%%%%%%%%%%%

\begin{document}

\title{Multi-style Generative Reading Comprehension}

\author{Kyosuke Nishida$^1$, 
Itsumi Saito$^1$, 
Kosuke Nishida$^1$, 
\\{\bf Kazutoshi Shinoda}$^2$\thanks{\ \ Work done during an internship at NTT.}, 
{\bf Atsushi Otsuka}$^1$,
{\bf Hisako Asano}$^1$, 
{\bf Junji Tomita}$^1$\\
  $^1$NTT Media Intelligence Laboratory, NTT Corporation \hspace{1.5em}  $^2$The University of Tokyo\\
  {\tt kyosuke.nishida@acm.org}
}

\date{}

\maketitle
\begin{abstract}
This study focuses on the task of multi-passage reading comprehension (RC) where an answer is provided in natural language. Current mainstream approaches treat RC by extracting the answer span from the provided passages and cannot generate an abstractive summary from the given question and passages. Moreover, they cannot utilize and control different styles of answers, such as concise phrases and well-formed sentences, within a model. 
In this study, we propose a style-controllable Multi-source Abstractive Summarization model for QUEstion answering, called Masque. The model is an end-to-end deep neural network that can generate answers conditioned on a given style. 
Experiments with MS MARCO 2.1 show that our model achieved state-of-the-art performance %in terms of Rouge-L 
on two tasks with different answer styles.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Question answering has been a long-standing research problem. Recently, \textit{reading comprehension} (RC), a challenge to answer a question given textual evidence provided in a document set, has received much attention. Here, current mainstream studies have treated RC as a process of extracting an answer span from one passage~\citep{RajpurkarZLL16,RajpurkarJL18} or multiple passages~\citep{JoshiCWZ17}, which is usually done by predicting the start and end positions of the answer~\citep{Yu18,DevlinCLT18}.

The demand for answering questions in natural language is increasing rapidly, and this has led to the development of smart devices such as Siri and Alexa. However, in comparison with answer span extraction, the \textit{natural language generation} (NLG) ability for RC has been less studied. While datasets such as MS MARCO~\citep{Bajaj18} have been proposed for providing abstractive answers in natural language, the state-of-the-art methods~\citep{WuWLHWLLL18,YanAAAI19} are based on answer span extraction, even for the datasets. Generative models such as S-Net~\citep{TanWYDLZ18} suffer from a dearth of training data to cover open-domain questions.

\begin{figure}[t!]
\centering
\includegraphics[width=.47\textwidth]{./images/masque_mixratio2.eps}
\caption{Visualization of how our model generates the answer. Given a style (\textbf{Top}: well-formed NLG, \textbf{Bottom}: concise Q\&A), our model chooses to generate words from a fixed vocabulary or copy words from the question and multiple passages at each decoding step.
}
\label{fig:mixratio}
\end{figure}
 
Moreover, to satisfy various information needs, intelligent agents should be capable of answering \textit{one} question in \textit{multiple} styles, such as concise phrases that do not contain the context of the question and well-formed sentences that make sense even without the context of the question. These capabilities complement each other; however, the methods used in previous studies cannot utilize and control different answer styles within a model.

In this study, we propose a generative model, called \textit{Masque}, for multi-passage RC. 
On the MS MARCO 2.1 dataset,
Masque achieves state-of-the-art performance on the dataset's two tasks, Q\&A and NLG, with different answer styles. 
The main contributions of this study are that our model enables the following two abilities.

\paragraph{Multi-source abstractive summarization based RC.} The first idea is to use a pointer-generator mechanism for multi-passage RC, which was originally proposed for text summarization~\citep{SeeLM17}. \citet{HasselqvistHK17} and \citet{McCannKXS18} had introduced 
its RNN-based mechanism
%its concept 
to query-based abstractive summarization and question answering, respectively; however, their models cannot handle multiple passages effectively. We extend the mechanism to a Transformer~\citep{VaswaniSPUJGKP17} based one that allows words to be generated from a fixed vocabulary and words to be copied from both the question and multiple passages. 

\paragraph{Style-controllable RC.} The second novel idea is to introduce a method to control multiple answer styles using a single model, taking advantage of multi-style answers to improve RC for all styles involved. We also extend the pointer-generator mechanism to a conditional decoder simply by introducing an artificial token corresponding to the target style, like~\citep{JohnsonSLKWCTVW17,TakenoNY17}. It controls the mixture weights over three probability distributions with the given style at each decoding step, as shown in Figure~\ref{fig:mixratio}.

\section{Problem Formulation}
\label{sec:problem}

The task considered in this paper, is defined as:
\begin{problem}
\label{prob:prob}
Given a question with $J$ words $x^q = \{x^q_1, \ldots, x^q_J\}$, a set of $K$ passages, where each $k$-th passage is composed of $L$ words $x^{p_k} = \{x^{p_k}_1, \ldots, x^{p_k}_{L}\}$, and an answer style $s$, an RC system %distinguishes whether the question is answered on the basis of the provided passages and 
outputs an answer $y = \{y_1, \ldots, y_T \}$ conditioned on the style.
%when the question can be answered.
\end{problem}
In short, for inference, given a set of 3-tuples $(x^q, \{x^{p_k}\}, s)$, the system predicts $P(y)$.
%and $P(a)$, where $a$ is $1$ if the question is answerable with the provided passages and $0$ otherwise.
The training data is a set of 6-tuples: $(x^q, \{x^{p_k}\}, s, y, a, \{r^{p_k}\})$, where 
$a$ is $1$ if the question is answerable with the provided passages and $0$ otherwise, and 
$r^{p_k}$ is $1$ if the $k$-th passage is required to formulate the answer and $0$ otherwise.

\section{Proposed Model} 

Our proposed model, \textit{Masque}, is based on \textbf{multi-source abstractive summarization}; the answer our model generates can be viewed as a summary from the question and multiple passages. It is also \textbf{style-controllable}; one model can generate the answer with the target style.


\begin{figure}[t!]
\centering
\includegraphics[width=.48\textwidth]{./images/Masque_model11.eps}
\caption{The Masque model architecture.}
\label{fig:model}
\end{figure}

Masque directly models 
the conditional probability $p(y|x^q, \{x^{p_k}\}, s)$.
%the conditional probabilities, $p(y|x^q, \{x^{p_k}\}, s)$ and $p(a|x^q, \{x^{p_k}\}, s)$. 
In addition to multi-style learning, it considers passage ranking and answer possibility classification together as multi-task learning in order to improve accuracy. 
Figure~\ref{fig:model} shows the model architecture. It consists of the following modules.
\begin{layer1}
The \textbf{question-passages reader} (\S\ref{sec:reader}) models interactions between the question and passages.
\end{layer1}
\begin{layer1}
The \textbf{passage ranker} (\S\ref{sec:ranker}) finds relevant passages to the question.
\end{layer1}
\begin{layer1}
The \textbf{answer possibility classifier} (\S\ref{sec:classifier}) identifies answerable questions.
\end{layer1}
\begin{layer1}
The \textbf{answer sentence decoder} (\S\ref{sec:decoder}) outputs a sequence of words conditioned on the style.
\end{layer1}


\subsection{Question-Passages Reader}
\label{sec:reader}

Given a question and  passages, the question-passages reader matches them so that the interactions among the question (passage) words conditioned on the passages (question) can be captured.

\subsubsection{Word Embedding Layer}

Let $x^q$ and $x^{p_k}$ represent 
one-hot vectors of 
words in the question and  $k$-th passage. First, this layer projects each of the one-hot vectors (of size $V$) into a $d_\mathrm{word}$-dimensional continuous vector space with a pre-trained weight matrix $W^e \in \mathbb{R}^{d_\mathrm{word} \times V}$ such as GloVe~\citep{PenningtonSM14}. Next, it uses contextualized word representations, ELMo~\citep{PetersNIGCLZ18}, which is a character-level two-layer bidirectional language model pre-trained on a large-scale corpus. ELMo representations allow our model to use morphological clues to form robust representations for out-of-vocabulary words unseen in training. Then, the concatenation of the word and contextualized embedding vectors is passed to a two-layer highway network~\citep{SrivastavaGS15} that is shared for the question and passages. 


\subsubsection{Shared Encoder Layer}
\label{sec:tfenc}

%Note that the input of this layer is a vector of dimension p1 + p2 = 500 for each individual word, which is immediately mapped to d = 128 by a one-dimensional convolution. T

This layer uses a stack of Transformer blocks, which are shared for the question and passages, on top of the embeddings provided by the 
word embedding layer. The input of the first block is immediately 
mapped to a $d$-dimensional vector
%of \textcolor{red}{dimension $d$} 
by a linear transformation. %one-dimensional convolution. 
The outputs of this layer are sequences of $d$-dimensional vectors: $E^{p_k} \in \mathbb{R}^{d \times L}$ for the $k$-th passage and $E^q \in \mathbb{R}^{d \times J}$ for the question.

\paragraph{Transformer encoder block.} It consists of two sub-layers: a self-attention layer and a position-wise 
feed-forward network. For the self-attention layer, we adopt the multi-head attention mechanism defined in \citep{VaswaniSPUJGKP17}. The feed-forward network consists of two linear transformations with a GELU~\citep{HendrycksG16} activation in between, following OpenAI GPT~\citep{RadfordNSS18}.
%and BERT~\citep{DevlinCLT18}. 
Each sub-layer is placed inside a residual block~\citep{HeZRS16}. For an input $x$ and a given sub-layer function $f$, the output is $\mathrm{LayerNorm}(f(x)+x)$, where $\mathrm{LayerNorm}$ indicates the layer normalization proposed in~\citep{BaKH16}.
To facilitate these residual connections, all sub-layers produce outputs of dimension $d$. Note that our model does not use any position embeddings because  ELMo gives the positional information of the words in each sequence.

\subsubsection{Dual Attention Layer}
\label{sec:dual}

This layer fuses information from the passages to the question as well as from the question to the passages in a dual mechanism. 

It first computes a similarity matrix $U^{p_k} \in \mathbb{R}^{L{\times}J}$ between the question and $k$-th passage, as is done in \citep{SeoKFH17}, where
\begin{align}
U^{p_k}_{lj} = {w^a}^\top [ E^{p_k}_l; E^q_j; E^{p_k}_l \odot E^q_j ]
\end{align}
indicates the similarity between the $l$-th word of the $k$-th passage and the $j$-th question word. $w^a \in \mathbb{R}^{3d}$ are learnable parameters. The $\odot$ operator denotes the Hadamard product, and the $[;]$ operator means vector concatenation across the rows. Next, it obtains the row and column normalized similarity matrices $A^{p_k} = \mathrm{softmax}_j({U^{p_k}}^\top) \in \mathbb{R}^{J\times L}$ and $B^{p_k}  = \mathrm{softmax}_{l}(U^{p_k}) \in \mathbb{R}^{L \times J}$. We use DCN~\citep{XiongZS17} as the dual attention mechanism to obtain question-to-passage representations $G^{q \rightarrow p_k} \in \mathbb{R}^{5d \times L}$:
\begin{align}
\nonumber
[E^{p_k}; \bar{A}^{p_k}; \bar{\bar{A}}^{p_k}; E^{p_k} \odot \bar{A}^{p_k}; E^{p_k} \odot \bar{\bar{A}}^{p_k}] 
\end{align}
and passage-to-question ones
%representations 
$G^{p \rightarrow q} \in \mathbb{R}^{5d \times J}$:
%\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts
\begin{align}
\begin{split}
\nonumber
& [ E^{q} ; \max_k(\bar{B}^{p_k}); \max_k(\bar{\bar{B}}^{p_k}); \\
&\hspace{1em} E^{q} \odot \max_k(\bar{B}^{p_k}); E^{q} \odot \max_k(\bar{\bar{B}}^{p_k})  ] \mathrm{\ \ where}
\end{split}\\
\nonumber
&\bar{A}^{p_k} =  E^q A^{p_k}\in \mathbb{R}^{d \times L}, \ 
\bar{B}^{p_k} =  E^{p_k} B^{p_k} \in \mathbb{R}^{d \times J} \\
\nonumber
&\bar{\bar{A}}^{p_k} = \bar{B}^{p_k} A^{p_k} \in \mathbb{R}^{d \times L}, \ 
\bar{\bar{B}}^{p_k} = \bar{A}^{p_k} B^{p_k} \in \mathbb{R}^{d \times J}.
\end{align}
%\endgroup

\subsubsection{Modeling Encoder Layer}

This layer uses a stack of Transformer encoder blocks for question representations and obtains $M^q \in \mathbb{R}^{d \times J}$ from $G^{p \rightarrow q}$. It also uses an another stack for passage representations and obtains $M^{p_k} \in \mathbb{R}^{d \times L}$ from $G^{q \rightarrow p_k}$ for each $k$-th passage. The outputs of this layer, $M^q$ and $\{M^{p_k}\}$, are passed on to the answer sentence decoder; the $\{M^{p_k}\}$ are also passed on to the passage ranker and answer possibility classifier.

\subsection{Passage Ranker}
\label{sec:ranker}

The passage ranker maps the output of the modeling layer,  $\{M^{p_k}\}$, to the relevance score of each passage. To obtain a fixed-dimensional pooled representation of each passage sequence, this layer takes the output for the first passage word, $M^{p_k}_1$, which corresponds to the beginning-of-sentence token. It calculates the relevance of each $k$-th passage to the question as:
\begin{align}
\beta^{p_k} = \mathrm{sigmoid}({w^r}^\top M^{p_k}_1),
\end{align}
where $w^r \in \mathbb{R}^{d}$ are learnable parameters.

\subsection{Answer Possibility Classifier}
\label{sec:classifier}

The answer possibility classifier maps the output of the modeling layer,  $\{M^{p_k}\}$,  to the probability of the answer possibility. The classifier takes the output for the first word, $M^{p_k}_1$, for all passages and concatenates them to obtain a fixed-dimensional representation.  It calculates the answer possibility to the question as:
%\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts
\begin{align}
P(a) = \mathrm{sigmoid}({w^c}^\top [M^{p_1}_1; \ldots; M^{p_K}_1]),
\end{align}
%\endgroup
where $w^c \in \mathbb{R}^{Kd}$ are learnable parameters.

\subsection{Answer Sentence Decoder}
\label{sec:decoder}

Given the outputs 
provided by the reader, 
the decoder generates a sequence of answer words one element at a time. It is auto-regressive~\citep{Graves13}, consuming the previously generated words as additional input at each decoding step.

\subsubsection{Word Embedding Layer}

Let $y = \{y_1, \ldots, y_{T}\}$ represent one-hot vectors of words in the answer. This layer has the same components as the word embedding layer of the question-passages reader, except that it uses a unidirectional ELMo in order to ensure that the predictions for position $t$ depend only on the known outputs at positions less than $t$.

Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ($y_1$), like~\citep{TakenoNY17}.
At test time, the user can specify the first token to control the answer styles. This modification does not require any changes to the model architecture.  Note that introducing the tokens on the decoder side prevents the passage ranker and answer possibility classifier from depending on the answer style.


\subsubsection{Attentional Decoder Layer}
\label{sec:style}

This layer uses a stack of Transformer decoder blocks on top of the embeddings provided by the word embedding layer. The input is immediately mapped to a $d$-dimensional vector by a linear transformation, and the output of this layer is a sequence of $d$-dimensional vectors: $\{s_1, \ldots, s_T\}$.

\paragraph{Transformer decoder block.} 
In addition to the encoder block, this block consists of second and third sub-layers after the self-attention block and before the feed-forward network, as shown in Figure~\ref{fig:model}. As in \citep{VaswaniSPUJGKP17}, the self-attention sub-layer uses a sub-sequent mask to prevent positions from attending to subsequent positions. The second and third sub-layers perform the multi-head attention over $M^q$ and $M^{p_\mathrm{all}}$, respectively. 
The $M^{p_\mathrm{all}}$ is the concatenated outputs of the encoder stack for the passages, 
\begin{align}
M^{p_\mathrm{all}} = [M^{p_1}, \ldots, M^{p_K}] \in \mathbb{R}^{d \times KL}.
\end{align}
 The $[,]$ operator means vector concatenation across the columns. This attention for the concatenated passages enables our model to produce attention weights that are comparable between passages.


\subsubsection{Multi-source Pointer-Generator}
\label{sec:copy}

Our extended %pointer-generator 
mechanism allows both 
words to be generated from a fixed vocabulary and words to be copied from both the question and multiple passages. Figure~\ref{fig:copy} shows the overview.

\begin{figure}[t!]
\centering
\includegraphics[width=.44\textwidth]{./images/Masque_copy3.eps}
\caption{Multi-source pointer-generator mechanism. Weights $\lambda^v, \lambda^q, \lambda^p$ for the probability of generating words from the vocabulary and copying words from the question and the passages are calculated for each decoding step. The three distributions are weighted and summed to obtain the final distribution.}
\label{fig:copy}
\end{figure}

\paragraph{Extended vocabulary distribution.}
Let the extended vocabulary,  $V_\mathrm{ext}$, be the union of the common words (a small subset of the full vocabulary, $V$, defined by the reader-side word embedding matrix) and all words appearing in the input question and passages. $P^v$ denotes the probability distribution of the $t$-th answer word, $y_t$, over the extended vocabulary. It is defined as:
\begin{align}
P^v(y_t)  =\mathrm{softmax}({W^2}^\top (W^1 s_t  + b^1)),
\end{align}
where the output embedding $W^2 \in \mathbb{R}^{d_\mathrm{word} \times V_\mathrm{ext}}$ is tied with the corresponding part of the input embedding~\citep{InanKS17}, and $W^1 \in \mathbb{R}^{d_\mathrm{word} \times d}$ and $b^1 \in \mathbb{R}^{d_\mathrm{word}}$ are learnable parameters. $P^v(y_t)$ is zero if $y_t$ is an out-of-vocabulary word for $V$.

\paragraph{Copy distribution.}
The copy mechanism used in the original pointer-generator is based on the attention weights of a single-layer attentional RNN decoder~\citep{SeeLM17}. The attention weights in our decoder stack are the intermediate outputs in multi-head attentions and are not suitable for the copy mechanism. Therefore, our model also uses additive attentions for the question and multiple passages on top of the decoder stack. 

The layer takes $s_t$ as the query and outputs $\alpha^q_t \in \mathbb{R}^J$ ($\alpha^p_t \in \mathbb{R}^{KL}$) as the attention weights and $c^q_t \in \mathbb{R}^d$ ($c^p_t \in \mathbb{R}^d$) as the context vectors for the question (passages):
\begin{align}
e^q_j &= {w^q}^\top \tanh(W^{qm} M_j^q + W^{qs} s_t +b^q), \\
\alpha^q_t &= \mathrm{softmax}(e^q), \\ 
c^q_t &= \textstyle \sum_j \alpha^q_{tj} M_j^q, \\
%\end{align}
%\begin{align}
e^{p_k}_l &= {w^p}^\top \tanh(W^{pm} M_l^{p_k} + W^{ps} s_t +b^p), \\
\alpha^p_t &= \mathrm{softmax}([e^{p_1}; \ldots; e^{p_K}]), \\
c^p_t &=  \textstyle \sum_{l} \alpha^p_{tl} M^{p_\mathrm{all}}_{l}, 
\end{align}
where $w^q$, $w^p \in \mathbb{R}^d$, 
$W^{qm}$, $W^{qs}$, $W^{pm}$, $W^{ps}  \in \mathbb{R}^{d \times d}$, and $b^q$, $b^p \in \mathbb{R}^d$ are learnable parameters.

$P^q$ and $P^p$ are the copy distributions over the extended vocabulary, defined as:
%\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts
\begin{align}
P^q(y_t) &=  \textstyle \sum_{j: x^q_j = y_t} \alpha^q_{tj}, \\
P^p(y_t) &= \textstyle \sum_{l: x^{p_{k(l)}}_{l} = y_t} \alpha^p_{tl},
\end{align}
%\endgroup
where $k(l)$ means the passage index corresponding to the $l$-th word in the concatenated passages.

%, as shown in~Figure~\ref{fig:copy}

\paragraph{Final distribution.}
The final distribution of the $t$-th answer word, $y_t$, is defined as a mixture of the three distributions:
\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts
\begin{align}
P(y_t) = \lambda^v P^v(y_t) +  \lambda^q P^q(y_t) + \lambda^p P^p(y_t),
\end{align}
\endgroup
where the mixture weights are given by 
\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts
\begin{align}
\lambda^v, \lambda^q, \lambda^p = \mathrm{softmax}(W^m [s_t; c^q_t; c^p_t] + b^m).
\end{align}
\endgroup
$W^m \in \mathbb{R}^{3 \times 3d}$, $b^m \in \mathbb{R}^3$ are learnable parameters.


\subsubsection{Combined Attention}
\label{sec:combined}

In order not to use words in irrelevant passages, our model introduces the concept of combined attention~\citep{SunHLLMT18}. While the original technique combines the word and sentence level attentions, our model combines the passage-level relevance $\beta^{p_k}$ and word-level attentions $\alpha^p_t$ by using simple scalar multiplication and re-normalization. The updated word attention is:
\begin{align}
\alpha^p_{tl} & := \frac{\alpha^p_{tl} \beta^{p_{k(l)} }}{\sum_{l'} \alpha^p_{tl'} \beta^{p_{k(l')}}}.
\end{align}


\subsection{Loss Function}

We define the training loss as the sum of losses in 
\begin{align}
L(\theta) = L_\mathrm{dec} + \gamma_\mathrm{rank} L_\mathrm{rank} + \gamma_\mathrm{cls} L_\mathrm{cls}
\end{align}
where $\theta$ is the set of all learnable parameters, and $\gamma_\mathrm{rank}$ and $\gamma_\mathrm{cls}$ are balancing parameters.

The loss of the decoder, $L_\mathrm{dec}$, is the negative log likelihood of the whole target answer sentence averaged over $N_\mathrm{able}$ answerable examples:
\begin{align}
L_\mathrm{dec} = - \frac{1}{N_\mathrm{able}}\sum_{(a,y)\in \mathcal{D}} \frac{a}{T} \sum_t \log P(y_{t}),
\end{align}
where $\mathcal{D}$ is the training dataset.


The losses of the passage ranker, $L_\mathrm{rank}$, and
the answer possibility classifier, $L_\mathrm{cls}$, are
the binary cross entropy between the true and predicted values averaged over all $N$ examples:
\begingroup\makeatletter\def\f@size{9.5}\check@mathfonts
\begin{gather}
L_\mathrm{rank} = -  \frac{1}{NK} \sum_k \sum_{r^{p_k}\in\mathcal{D}}  
\biggl(
\begin{split}
&r^{p_k} \log \beta^{p_k} +  \\
&(1-r^{p_k}) \log (1-\beta^{p_k}) 
\end{split}
\biggr),\\
L_\mathrm{cls} = - \frac{1}{N} \sum_{a \in \mathcal{D}} 
\biggl(
\begin{split}
&a \log P(a) + \\
&(1-a) \log (1-P(a)) 
\end{split}
\biggr).
\end{gather}
\endgroup


\section{Experiments}

\subsection{Setup}


\begin{table}[t!]
\centering
{\small \tabcolsep=5pt
\begin{tabular}{c|ccc}
\hline
set   & train & dev. & eval \\ \hline
ALL & 808,731 & 101,093 & 101,092\\
ANS & 503,370 & 55,636 & --\\
WFA & 153,725 & 12,467 & --\\
\hline
\end{tabular} \\
}
\caption{Numbers of question-answer pairs used in the experiments. ALL, ANS, and WFA mean all, answerable, and well-formed data, respectively.}
\label{tb:data}
\end{table}


\paragraph{Datasets and styles.}
We conducted experiments on the two tasks
of MS MARCO 2.1~\citep{Bajaj18}.
The answer styles considered in the experiments corresponded to the two tasks.
The  \textbf{NLG} task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words.
The \textbf{Q\&A}  task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question.
For instance, for the question ``tablespoon in cup'', 
the answer in the Q\&A task will be ``16'', and 
the answer in the NLG task will be ``There are 16 tablespoons in a cup.''
In addition to the \textbf{ALL} dataset, we prepared two subsets (Table~\ref{tb:data}). The \textbf{ANS} set consists of answerable questions, and the \textbf{WFA} set consists of the answerable questions and well-formed answers, where WFA $\subset$ ANS $\subset$ ALL. 


\paragraph{Model configurations.}
We trained our model on a machine with eight NVIDIA P100 GPUs. Our model was jointly trained with the two answer styles in the ALL set for a total of eight epochs with a batch size of 80. The training took roughly six days.  The ensemble model consists of six training runs with the identical architecture and hyperparameters.
The hidden size $d$ was 304, and the number of attention heads was 8. The inner state size of the feed-forward networks was 256. The numbers of shared encoding blocks, modeling blocks for question, modeling blocks for passages, and decoder blocks were 3, 2, 5, and 8, respectively. We used the pre-trained uncased 300-dimensional 
GloVe~\citep{PenningtonSM14}
%\footnote{\url{https://nlp.stanford.edu/projects/glove/}} 
and the original 512-dimensional 
ELMo~\citep{PetersNIGCLZ18}.
%\footnote{\url{https://allennlp.org/elmo}}. 
We used the spaCy tokenizer, and all words were lowercased except the input for ELMo. The number of common words 
in $V_\mathrm{ext}$
%in the extended vocabulary 
was 5,000.

\paragraph{Optimizer.}
We used the Adam optimization~\citep{KingmaB15} with $\beta_1 = 0.9$, $\beta_2 = 0.999$, and $\epsilon = 10^{-8}$. Weights were initialized using $N(0, 0.02)$, except that the biases of all the linear transformations were initialized with zero vectors. The learning rate was increased linearly from zero to $2.5 \times 10^{-4}$ in the first 2,000 steps and annealed to 0 using a cosine schedule. All parameter gradients were clipped to a maximum norm of $1$. An exponential moving average was applied to all trainable variables with a decay rate 0.9995. The balancing factors of joint learning, $\lambda_\mathrm{rank}$ and $\lambda_\mathrm{cls}$, were set to 0.5 and 0.1.

\paragraph{Regularization.}

We used a modified version of the L$_2$ regularization proposed in~\citep{LoshchilovH17}, with $w = 0.01$.
%on all non bias. 
We additionally used a dropout~\citep{SrivastavaHKSS14} rate of 0.3 for all highway networks and residual and scaled dot-product attention operations in the multi-head attention mechanism. We also used one-sided label smoothing~\citep{SzegedyVISW16} for the passage relevance and answer possibility labels. We smoothed only the positive labels to 0.9.

\subsection{Results} 

\paragraph{Does our model achieve state-of-the-art performance for generative RC?}

Table~\ref{tb:nlg-leaderboard} shows that 
our ensemble model, controlled with the NLG and Q\&A styles, achieved state-of-the-art performance on the NLG and Q\&A tasks in terms of Rouge-L.  In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1.
The capability of creating abstractive summaries from the question and passages contributed to its improvements over the state-of-the-art extractive approaches~\citep{WuWLHWLLL18,YanAAAI19}.  

\begin{table}
\centering
{\small \tabcolsep=1.4pt
\begin{tabular}{l|cc|cc}
\hline
 & \multicolumn{2}{c|}{NLG} & \multicolumn{2}{c}{Q\&A}\\ %\hline
Model & Rouge-L & Bleu-1 & Rouge-L & Bleu-1\\ \hline
BiDAF \citeyearpar{SeoKFH17} & 16.91 & 9.30 & 23.96 & 10.64 \\
Deep Cascade QA \citeyearpar{YanAAAI19} & 35.14 & 37.35 &  52.01 & {\bf 54.64}\\ 
S-Net \citeyearpar{TanWYDLZ18}\footnote{An unpublished variant by Bo Shao of SYSU University.}   & 45.04 & 40.62 & 44.96 & 46.36\\
VNET \citeyearpar{WuWLHWLLL18}  & 48.37 & 46.75 & 51.63 & 54.37\\ \hline
Masque (NLG; single) & 49.19 &  49.63 & 48.42 & 48.68\\
Masque (Q\&A; single) & 25.66 & 36.62 & 50.93 & 42.37\\ 
%Masque (single) & 49.19 &  49.63 & 50.93 & 42.37\\ 
\hline
Masque (NLG; ensemble) & {\bf 49.61} & {\bf 50.13} & 48.92& 48.75 \\
Masque (Q\&A; ensemble) & 28.53 & 39.87 & {\bf 52.20} & 43.77 \\ 
%Masque (ensemble) & {\bf 49.61} & {\bf 50.13} & {\bf 52.20} & 43.77 \\ 
\hline
Human Performance & 63.21 & 53.03  & 53.87 & 48.50\\
\hline
\end{tabular}
}
\caption{Performance of our model and competing models on the MS MARCO 2.1 leaderboard (3 January 2019). 
Whether the competing models are ensemble models or not is unreported. }
\label{tb:nlg-leaderboard}
\end{table}

\begin{table}[t!]
\centering
{\small \tabcolsep=1.5pt %3.5pt
\begin{tabular}{p{15em}|c|cc}
\hline
Model & train & Rouge-L & Bleu-1 \\ \hline
Masque (NLG style; single)
%\footnote{It was trained with all data (ALL train set). } 
& ALL&  {\bf 69.77} & {\bf 65.56} \\ %\hline
\begin{tabular}{p{15em}}
w/o multi-style learning (\S\ref{sec:style})
%\footnote{It was trained with well-formed data (WFA train set).}
\end{tabular} & WFA &68.20 & 63.95 \\
\begin{tabular}{p{15em}}
%\hspace{1em}
$\hookrightarrow$
w/o Transformer (\S\ref{sec:tfenc}, \S\ref{sec:style})%\footnotemark[3] %WFA
\end{tabular} & WFA & 67.13 & 62.96 \\ 
\begin{tabular}{p{15em}}
w/o passage ranker (\S\ref{sec:ranker})%\footnotemark[3] % WFA
\end{tabular} & WFA & 68.05 & 63.82 \\
\begin{tabular}{p{15em}}
w/o possibility classifier (\S\ref{sec:classifier})
%\footnote{It was trained with answerable data (ANS train set).}
\end{tabular} & ANS & 69.64 & 65.41 \\ \hline
Masque w/ gold passage ranker & ALL & 78.70 & 78.14 \\ 
\hline
\end{tabular}
}
\caption{RC performance of our models for Rouge-L and Bleu-1 on the WFA dev.~set. The models were trained with the dataset described in the column 'train'.}
\label{tb:ablation}
\end{table}

\paragraph{Does our multi-style learning improve NLG performance?}

Table~\ref{tb:ablation} shows the results of the ablation test for our model (controlled with the NLG style) on the well-formed answers of the WFA dev.~set. Our model, which was trained with the 
ALL set consisting of the two styles,
outperformed the model trained with the WFA set consisting of the single style.
Multi-style learning allowed our model to improve NLG performance by also using non-sentence answers.

\paragraph{Does our Transformer-based pointer-generator improve NLG performance?}

Table~\ref{tb:ablation} shows that our model outperformed the model that used RNNs and self-attentions instead of Transformer blocks as in MCAN~\citep{McCannKXS18}.
Our deep Transformer decoder captured the interaction among the question, the passages, and the answer better than a single-layer LSTM decoder.

\paragraph{Does our joint learning with the ranker and classifier improve NLG performance?}

Table~\ref{tb:ablation} shows that 
our model (jointly trained with the passage ranker and answer possibility classifier) outperformed the model
that did not use the ranker and classifier.
The joint learning has a regularization effect on the question-passages reader.

We also confirmed that the gold passage ranker, which can predict passage relevances perfectly, improves RC performance significantly. Passage re-ranking will be a key to developing a system that can outperform humans.

\paragraph{Does our joint learning improve the passage re-ranking performance?}

\begin{table}[t!]
\centering
{\small \tabcolsep=3.5pt
\begin{tabular}{p{14em}|c|cc}
\hline
Model & train & MAP & MRR \\ \hline
Bing (initial ranking) & - & 34.62 & 35.00 \\ \hline
Masque (single) &ALL & {\bf 69.51} & {\bf 69.96}\\ %ALL
\begin{tabular}{p{14em}}
w/o answer decoder (\S\ref{sec:decoder})
\end{tabular} & ALL & 67.03 & 67.49 \\ 
\begin{tabular}{p{14em}}
w/o multi-style learning (\S\ref{sec:style})
\end{tabular} & WFA & 65.51 & 65.59 \\ 
\begin{tabular}{p{14em}}
w/o possibility classifier (\S\ref{sec:classifier})
\end{tabular} & ANS & 69.08 & 69.54 \\ 
\hline
\end{tabular}
}
\caption{Passage re-ranking performance for MAP and MRR~\citep{CraswellR09,Craswell09a} on the ANS dev.~set. }
\label{tb:ranker}
\end{table}

\begin{figure}[t!]
\centering
\includegraphics[width=.35\textwidth]{./images/masque_prcurve333.eps}
\caption{Precision-recall curve of answer possibility classification on the ALL dev.~set.}
\label{fig:anspos}
\end{figure}

Table~\ref{tb:ranker} shows the passage re-ranking performance for the ten given passages on the ANS dev.~set. Our ranker improved the initial ranking provided by Bing by a significant margin. Also, the ranker shares the question-passages reader with the answer decoder, and this sharing contributed to the improvements over the ranker trained without the answer decoder. 
This result is similar to those reported in \citep{NishidaSOAT18}.
Moreover, the joint learning with the answer possibility classifier and multiple answer styles, which enables our model to learn from a larger number of data, improved the re-ranking. 
 
\paragraph{Does our model accurately identify answerable questions?}

Figure~\ref{fig:anspos} shows the precision-recall curve of answer possibility classification on the ALL dev.~set, where the positive class is the answerable data. Our model identified the answerable questions well. The maximum $F_1$ score was 0.7893. 
This is the first report on answer possibility classification with MS MARCO 2.1.

\paragraph{Does our model accurately control answers with different styles?}
Figure~\ref{fig:length} shows the lengths of the answers generated by our model, which are broken down by answer style and query type.
The generated answers were relatively shorter than the reference answers but well controlled with the target style in every query type.

Also, we should note that our model does not guarantee the consistency in terms of meaning across the answer styles. We randomly selected 100 questions and compared the answers our model generated with the NLG and Q\&A styles.  
The consistency ratio was 0.81, where major errors were due to copying words from different parts of the passages and generating different words, especially yes/no, from a fixed vocabulary.

\paragraph{Error analysis.}

Appendix~\ref{sec:examples} shows 
examples of generated answers.
We found %some important errors: 
(d) style errors;
(e) yes/no classification errors;
(f) copy errors with respect to numerical values; and
(c,e) grammatical errors that were originally contained in the inputs. %query and passages. 


\begin{figure}[t!]
\centering
\includegraphics[width=.46\textwidth]{./images/length3.eps}
\caption{Lengths of answers generated by Masque broken down by answer style and query type on the WFA dev.\ set. Error bars mean standard errors.}
\label{fig:length}
\end{figure}

\section{Related Work and Discussion}

\paragraph{RC with NLG.}

MCAN~\citep{McCannKXS18} 
frames various tasks as question answering tasks that take a 3-tuple (question, context, answer) as inputs. It uses a pointer-generator decoder to jointly learn all tasks without any task-specific parameters; unlike ours, it cannot modify answers with the target style or handle multiple passages.
S-Net \citep{TanWYDLZ18} uses a generative model for multi-passage RC. It uses answer extraction to predict the most important spans from the passage as evidence; then it uses the evidence to generate the final answers. However, it 
%cannot copy words from the question and passages. 
does not handle the extended vocabulary in order to generate words appearing in the question and passages. 

To the best of our knowledge, there are no datasets for providing answers in natural language with multiple styles except MS MARCO 2.1, although there are some datasets that provide abstractive answers. 
DuReader~\citep{HeLLLZXLWWSLWW18}, a Chinese multi-document RC dataset, provides the top-10 ranked entire documents from Baidu Search and Zhidao. Many of the answers are long and relatively far from the source documents compared with those from MS MARCO.
NarrativeQA~\citep{KociskySBDHMG18} proposed a dataset about stories or summaries of books or movie scripts. The documents are long, averaging 62,528 (659) words in stories (summaries), while the answers are relatively short, averaging 4.73 words. 
Moreover, DuoRC~\citep{KhapraSSA18} and CoQA~\citep{ReddyCM18} contain abstractive answers;  most of the answers are 
%entities or 
short phrases.

%DuoRC~\citep{KhapraSSA18} is a dataset about movie strories, where QAs are created from different versions of a document narrating the same underlying story.  Most of the answers are very short (a sinlge word or a short phrase) \textcolor{red}{CoQA~\citep{ReddyCM18}} contains multi-turn QAs obtained from conversations about text passages. The questions and answers are shorter than those of SQuAD due to the nature of conversatinal QA.

\paragraph{Controllable text generation.}

Many studies have been carried out in the framework of style transfer, which is the task of rephrasing the text so that it contains specific styles such as sentiment. Recent work uses artificial tokens~\citep{SennrichHB16,JohnsonSLKWCTVW17}, variational auto-encoders~\citep{HuYLSX17}, adversarial training~\citep{%ShenLBJ17,
FuTPZY18,TsvetkovBSP18}, or prior knowledge~\citep{LiJHL18} to separate the content and style on the encoder side. 
On the decoder side, conditional language modeling has been used to generate output sentence with the target style.
In addition to style transfer, output length control with conditional language modeling has been well studied~\citep{KikuchiNSTO16,TakenoNY17,FanGA18}. Our style-controllable RC relies on conditional language modeling on the decoder side. 

\paragraph{Multi-passage RC.} 

%Several models capable of reading multiple passages retrieved by a search engine have been proposed. 
The simplest approach is to concatenate the passages and find the answer from the concatenated one as in \citep{WangYWCZ17}. Earlier pipeline models find a small number of relevant passages with a TF-IDF based ranker and pass them to a neural reader~\citep{ChenFWB17,GardnerC18}, while more recent pipeline 
models use a neural re-ranker to more accurately select the relevant  passages~\citep{WangAAAI2018,NishidaSOAT18}. 
Also, non-pipelined models (including ours) consider all the provided passages and find the answer by comparing scores between passages~\citep{TanWYDLZ18,WuWLHWLLL18}.  The most recent models make a proper trade-off between efficiency and accuracy~\citep{YanAAAI19,MinZSX18}.

\paragraph{RC with unanswerable question identification.}

%Identifying unanswerable questions is currently a popular topic in RC. 
The previous work of \citep{LevySCZ17,GardnerC18} outputs a no-answer score depending on the probability of all answer spans. \citeauthor{HuWPHYZ18}~\shortcite{HuWPHYZ18} proposed an answer verifier to compare the answer sentence with the question. \citeauthor{SunLQL18}~\shortcite{SunLQL18} proposed a unified model that jointly learns an RC model and an answer verifier. Our model introduces a classifier on the basis of question-passages matching, which is not dependent on the generated answer, unlike the previous methods.
%We need to investigate approaches for verifying abstractive summaries created from the question and passages.

\paragraph{Abstractive summarization.}

Current state-of-the-art models use pointer-generator mechanisms~\citep{%GuLLL16,
SeeLM17}.
In particular, content selection approaches, which decide what to summarize, have recently been used with abstractive models. Most methods select content at the sentence level~\citep{HsuLLMTS18,ChenB18} and the word level \citep{PasunuruB18,LiXLG18,GehrmannDR18}; our model incorporates content selection at the passage level in the combined attention. 

Query-based abstractive summarization has been rarely studied. \citet{NemaKLR17} proposed an attentional encoder-decoder model, and \citet{KhapraSSA18} reported that it performed worse than BiDAF %~\citep{SeoKFH17}
on DuoRC.
\citet{HasselqvistHK17} proposed a pointer-generator based model; however, it does not consider copying words from the question and multiple passages.

%\paragraph{Potential applications.}
%Our model is promising for knowledge-grounded conversation~\citep{GhazvininejadBC18,KielaWZDUS18} in order to generate a response in natural language (i.e., answer), consider the copy from the conversation history (i.e., question) and relevant facts (passages), and control conversation characteristics such as personality and speaking style.


\section{Conclusion}
We believe our study makes two contributions to the study of multi-passage RC with NLG. Our model enables 1) multi-source abstractive summarization based RC and 2) style-controllable RC. The key strength of our model is its high accuracy of generating abstractive summaries from the question and passages; our model achieved state-of-the-art performance in terms of Rouge-L on the Q\&A and NLG tasks of MS MARCO 2.1 that have different answer styles~\citep{Bajaj18}.

The styles considered in this paper are only related to the context of the question in the answer sentence; our model will be promising for controlling other styles such as length and speaking styles.
Future work will involve exploring the potential of hybrid models combining extractive and abstractive approaches and improving the passage re-ranking and answerable question identification.

%\fontsize{9.0pt}{10.0pt} \selectfont
\bibliographystyle{acl_natbib_nourl}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\newpage
\appendix
\onecolumn
\section{Reading Comprehension Examples generated by Masque from MS MARCO 2.1}
\label{sec:examples}

\begin{table*}[h!]
\centering
\caption{Our model could control answer styles appropriately for (a) natural language, (b) cloze-style, and (c) keywords questions. (d) The Q\&A was incorrect. (e) %Passage re-ranking worked poorly and 
The answers were not consistent between the styles. (f) Copying from numerical words worked poorly. There were some \underline{grammatical errors}.  }
\label{tb:examples}
{\footnotesize
\tabcolsep=1pt
\vspace{0.5pt}
\begin{tabular}{p{50em}}
\hline
\vspace{0.5pt}
\pbox{50em}{
\textbf{(a) Question}: why your body would feel like it is shaking\\
\textbf{Relevant Passage}: Shaking is a symptom in which a person has tremors (shakiness or small back and forth movements) in part or all of his body. Shaking can be due to cold body temperatures, rising fever (such as with infections), neurological problems, medicine effects, drug abuse, etc. ...Read more. \\
%\textbf{Passage Rank}: 1 \\
\textbf{Reference Answer (Q\&A)}: Shaking can be due to cold body temperatures, rising fever (such as with infections), neurological problems, medicine effects, drug abuse, etc.  \\
\textbf{Prediction (Q\&A)}: because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse~.~\cmark\\
\textbf{Reference Answers (NLG)}: Shaking can be due to cold body temperatures, rising fever, neurological problems, medicine effects and drug abuse. / 
Body would feel like it is shaking due to cold body temperatures, rising fever, neurological problems, medicine effects, drug abuse. \\\
\textbf{Prediction (NLG)}: your body would feel like it is shaking because of cold body temperatures , rising fever , neurological problems , medicine effects , drug abuse . \cmark
}
\vspace{1pt}
\\ \hline
\vspace{0.5pt}
\pbox{50em}{
\textbf{(b) Question}: \_\_\_\_\_ is the name used to refer to the era of legalized segregation in the united states \\
\textbf{Relevant Passage}: Jim Crow law, in U.S. history, any of the laws that enforced racial segregation in the South between the end of Reconstruction in 1877 and the beginning of the civil rights movement in the 1950s.
\\
%\textbf{Passage Rank}: 3 \\
\textbf{Reference Answer (Q\&A)}: Jim Crow \\
\textbf{Prediction (Q\&A)}: jim crow \cmark \\
\textbf{Reference Answer (NLG)}: Jim Crow is the name used to refer to the era of legalized segregation in the United States. \\
\textbf{Prediction (NLG)}: jim crow is the name used to refer to the era of legalized segregation in the united states . \cmark
}
\vspace{1pt}
\\ \hline
\vspace{0.5pt}
\pbox{50em}{
\textbf{(c) Question}: average height nba player\\
\textbf{Relevant Passage}: The average height of an NBA player is around 6 feet 7 inches. The tallest NBA player ever was Gheorghe Muresan, who was 7 feet 7 inches tall. In contrast, the shortest NBA player ever was Tyrone Muggsy Bogues, who was 5 feet 3 inches tall. \\
%\textbf{Passage Rank}: 1 \\
\textbf{Reference Answer (Q\&A)}: Around 6 feet 7 inches \\
\textbf{Prediction (Q\&A)}: 6 feet 7 inches	\cmark \\
\textbf{Reference Answers (NLG)}: The average height of NBA players is around 6 feet, 7 inches. / The height of NBA player is around 6 feet 7 inches.\\
\textbf{Prediction (NLG)}: the average height of \underline{an} national basketball association player is 6 feet 7 inches . \cmark
}
\vspace{1pt}
\\ \hline 
\vspace{0.5pt}
\pbox{50em}{
\textbf{(d) Question}: population of henryetta ok\\
\textbf{Relevant Passage}: With it's population of 5,927 people, Henryetta represented 14.79\% of the county's total. In addition, where the city of Henryetta has a population density of 891 people per square mile [344.2 people/km2], Okmulgee County averages 57.5 people per square mile [22.19 people/km2].\\
%\textbf{Passage Rank}: 1 \\
\textbf{Reference Answer (Q\&A)}: 5927 \\
\textbf{Prediction (Q\&A)}: the population of henryetta , oklahoma is 5,927 .   \cmark \hspace{.5em} (content is OK, but style is NG \xmark)\\
\textbf{Reference Answer (NLG)}: The population of Henryetta, Oklahoma is 5,927. \\
\textbf{Prediction (NLG)}:  the population of henryetta , oklahoma is 5,927 . \cmark
}
\vspace{1pt}
\\ \hline 
\vspace{0.5pt}
\pbox{50em}{
\textbf{(e) Question}: does gameplay programmer need math skill\\
%\textbf{Passage Rank}: 8 \\
\textbf{Relevant Passage}: A good computer programmer is more of a problem solver and logical thinker than a math buff. And besides, the industry is peppered with many successful computer programmers who do not really know much about mathematics. \\
\textbf{Reference Answer (Q\&A)}: No \\
\textbf{Prediction (Q\&A)}: yes \xmark \\
\textbf{Reference Answers (NLG)}: No, a gameplay programmer doesn't need math skill. / No, gameplay programmer do not need an math skill.\\
\textbf{Prediction (NLG)}: no , \underline{gameplay programmer} does not need math skill . \cmark
}
\vspace{1pt}
\\ \hline
\vspace{0.5pt}
\pbox{50em}{
\textbf{(f) Question}: how long does a freezer take to cool down\\
\textbf{Relevant Passage}: Quick Answer. It takes anywhere from three to 24 hours for a refrigerator to reach safe temperatures for storing food, depending on the size and type of unit. When the refrigerator compartment reaches 40 degrees Fahrenheit and the freezer reaches 5 degrees Fahrenheit, it is safe to transfer food items. Keep Learning. \\
%\textbf{Passage Rank}: 3 \\
\textbf{Reference Answer (Q\&A)}: 24 hours\\
\textbf{Prediction (Q\&A)}: 4 to 5 hours \xmark \\
\textbf{Reference Answers (NLG)}: A freezer takes 24 hours to cool down. / A  freezer take to cool down is 24 hours.\\
\textbf{Prediction (NLG)}: a freezer takes 4 to 12 hours to cool down . \xmark
}
\vspace{1pt}
\\ \hline
\end{tabular}
}
\end{table*}

\end{document}
