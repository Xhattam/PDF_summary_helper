Activation functions are a crucial component of neural networks
because they turn an otherwise linear classifier into a non-linear one, which has proven key to the high performances witnessed across a wide range of tasks in recent years. While different activation functions such as \sigmoid{} or \mytanh{} are often equivalent on a theoretical level, in the sense that they can all approximate arbitrary continuous functions \cite{Hornik:1991}, different activation functions often show very diverse behavior in practice.
%, when it comes to solving machine learning problems.  

For example, \sigmoid{}, one of the activation functions dominating in neural network practice for several decades
%, the so-called sigmoid function,  
eventually turned out less suitable for learning because (according to accepted wisdom) of its small derivative which may lead to vanishing gradients. 
%Much more suitable in this respect has proven the so-called ReLU function \cite{Glorot:2011}, 
In this respect, the so-called ReLU function \cite{Glorot:2011} has proven much more suitable.
It has an identity derivative in the positive region and is thus claimed to be less susceptible to vanishing gradients. It has therefore (arguably) become the most popular activation function. The recognition of ReLU's success has led to various extensions proposed \cite{Maas:2013,He:2015,Klambauer:2017}, but none has reached the same popularity, most likely because of ReLU's simplicity and because the gains reported tended to be inconsistent or marginal across datasets and models \cite{Ramach:2018}.

%Activation have been characterized by various properties ... However, [automatic search]
Activation functions have been characterized by a variety of properties deemed important for successful learning, such as ones %properties 
relating to their derivatives, monotonicity, and whether their range is finite or not. However, in recent work, \citet{Ramach:2018} employed automatic search to find high-performing novel activation functions, where their search space contained compositions of elementary unary and binary functions such as $\max$, $\min$, $\sin$, $\tanh$, or $\exp$. They found many functions violating properties deemed as useful, such as non-monotonic activation functions or functions violating the gradient-preserving property of ReLU. Indeed, their most successful function, which they call \swish{}, violates both of these conditions. However, as with previous works, they also only evaluated their newly discovered as well as their (rectifier) baseline activation functions on few different datasets, usually taken from the image classification community such as CIFAR \cite{Krizhevsky:2009} and ImageNet \cite{Russakovsky:2015}, and using few types of different networks, such as the deep convolutional networks abounding in the image classification community \cite{Szegedy:2016}.

To our best knowledge, there exists no large-scale empirical comparison of different activations across a variety of tasks and network architectures, and even less so within natural language processing (NLP).\footnote{An exception may be considered \citet{Xu:2015}, who, however, only contrast the rectifier functions on image classification datasets.} Thus, the question which activation function really performs best and most stably across different NLP tasks and popular NLP models remains unanswered to this date. 

In this work, we fill this gap. We compare (i) 21 different activation functions, including the 6 top performers found from automatic search in \citet{Ramach:2018}, across (ii) three popular NLP task types (sentence classification, document classification, sequence tagging) comprising 8 individual tasks, (iii) using three different popular NLP architectures, namely, MLPs, CNNs, and RNNs. We also (iv) compare all functions across two different dimensions, namely: top vs.\ average performance. 
%Top performance refers to the test result of the best performing model on the dev set, and is the usual metric reported in machine learning. Average performance refers to the test performance averaged across all differently hyperparametrized models. The latter score is useful when hyperparameter search is costly (or ignored for other reasons) because it indicates what can be expected with `average' hyperparameters.   

We find that %XYZ [also include LSTM]
a largely unknown activation function, \pentan{} \cite{Xu:2016}, performs most stably across our different tasks. We also find that it can successfully replace \mytanh{} and \sigmoid{} activations in LSTM cells. We further find that the majority of top performing functions found in \citet{Ramach:2018} do not perform well for our tasks. An exception is \swish{}, which performed well across several tasks, but less stably than \pentan{} and other functions.\footnote{Accompanying code to reproduce our experiments is available from \url{https://github.com/UKPLab/emnlp2018-activation-functions}.}

%[say that we're purely empirical and have to modes of comparison and why average is useful]