\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{bm}
\usepackage{array}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{url}
\usepackage{color}

\usepackage{verbatim}


\newcommand\BibTeX{B{\sc ib}\TeX}

\title{CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis}
% \author{
%   Mengting Hu \\
%   {\tt mthu@mail.nankai.edu.cn} \\
%   {Nankai University, Tianjin, China} \\
%   \And
%   Shiwan Zhao \\
%   {\tt zhaosw@cn.ibm.com }      \\
%   {IBM Research - China, Beijing , China}
%   }
\author{Mengting Hu\textsuperscript{1}\thanks{\quad This work was done when Mengting Hu was a research intern at IBM Research - China.}, Shiwan Zhao\textsuperscript{2}, Li Zhang\textsuperscript{2}, Keke Cai\textsuperscript{2},
Zhong Su\textsuperscript{2}, Renhong Cheng\textsuperscript{1}, Xiaowei Shen\textsuperscript{2} \\
\textsuperscript{1} Nankai University, \textsuperscript{2} IBM Research - China \\
mthu@mail.nankai.edu.cn, \{zhaosw, lizhang, caikeke, suzhong\}@cn.ibm.com, \\ chengrh@nankai.edu.cn, xwshen@cn.ibm.com
}

\begin{document}
\maketitle
\begin{abstract}
Aspect level sentiment classification is a fine-grained sentiment analysis task, compared to the sentence level classification. A sentence usually contains one or more aspects. To detect the sentiment towards a particular aspect in a sentence, previous studies have developed various methods for generating aspect-specific sentence representations. However, these studies handle each aspect of a sentence separately. In this paper, we argue that multiple aspects of a sentence are usually orthogonal based on the observation that different aspects concentrate on different parts of the sentence. To force the orthogonality among aspects, we propose constrained attention networks (CAN) for multi-aspect sentiment analysis, which handles multiple aspects of a sentence simultaneously. Experimental results on two public datasets demonstrate the effectiveness of our approach. We also extend our approach to multi-task settings, outperforming the state-of-the-arts significantly.
\end{abstract}
\section{Introduction}
Sentiment analysis \cite{Nasukawa2003Sentiment,liu2012sentiment}, an important task in natural language understanding, receives much attention in recent years. Aspect level sentiment classification is a fine-grained sentiment analysis task, which aims at detecting the sentiment towards a particular aspect in a sentence. A multi-aspect sentence (\emph{i.e.}, the sentence contains more than one aspect) can be categorized as {\bf overlapping} or {\bf non-overlapping}. A sentence is annotated as non-overlapping only if any two of its aspects have no overlap. One key observation is that around $85\%$ of the multi-aspect sentences are non-overlapping in the two public datasets. Figure \ref{sentence} shows a simple example. The non-overlapping sentence contains two aspects. The aspect \emph{food} is in the left part of the sentence while \emph{service} in the right part. Their distributions on words are {\bf orthogonal} to each other. Another observation is that only a few words relate to the opinion expression in each aspect. As shown in Figure \ref{sentence}, only the word \emph{``great''} is relevant to the aspect \emph{food} and \emph{``dreadful"} to \emph{service}. The distribution of the opinion expression of each aspect is {\bf sparse}. 

To detect the sentiment towards a particular aspect, previous studies \cite{Wang2016Attention,Ma2017Interactive,cheng2017aspect,ma2018targeted,huang2018aspect,wang2018learning} have developed various attention-based methods for generating aspect-specific sentence representations. To name a few, \cite{Wang2016Attention} proposes attention-based LSTMs for aspect level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. %By learning an aspect-specific representation of a sentence, they are more competitive for aspect level classification. 
\cite{wang2018learning} proposes a segmentation attention based LSTM model which can effectively capture the structural dependencies between the target and the sentiment expressions with a linear-chain conditional random field (CRF) layer. However, all these works are single-aspect sentiment analysis, which deals with aspects in a sentence one at a time, ignoring the orthogonality among multiple aspects. 

\begin{figure}
\setlength{\abovecaptionskip}{0.2cm}    %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=0.45\textwidth]{example.pdf}
\caption{Example of a non-overlapping sentence.} 
  \label{sentence} 
\end{figure}

Therefore, we propose a model for multi-aspect sentiment analysis, which handles multiple aspects of a sentence simultaneously. Specifically, we introduce orthogonal regularization for attention weights among multiple non-overlapping aspects. The orthogonal regularization tends to make the attention weights of multiple aspects concentrate on different parts of the sentence with less overlap. We also introduce the sparse regularization, which tends to make the attention weights of each aspect concentrate only on a few words. We call our networks with such regularizations {\bf constrained attention networks} (CAN). The implementation of adding regularization terms to attention weights of multiple aspects is similar to adding the penalization term in self-attention in \cite{lin2017structured}. The details will be introduced in the model section.

In addition to aspect level sentiment classification ({\bf ALSC}), aspect category detection ({\bf ACD}) is another task of aspect based sentiment analysis. We introduce ACD as an auxiliary task to assist the ALSC task, benefiting from the shared context of the two tasks. Aspect category detection~\cite{Zhou2015Representation,Schouten2018Supervised} is a task which aims to identify the aspect categories discussed in a given sentence from a predefined set of aspect categories (e.g., price, food, service). Take Figure \ref{sentence} as an example, aspect categories \emph{food} and \emph{service} are mentioned. We also apply our attention constraints to the ACD task. By applying attention weight constraints to both ALSC and ACD tasks in an end-to-end network, we further evaluate the effectiveness of CAN in multi-task settings.

In summary, the main contributions of our work are as follows:
\begin{itemize}
\vspace{-0.2cm}
\item We propose CAN for multi-aspect sentiment analysis. Specifically, we introduce {\bf orthogonal} and {\bf sparse} regularizations to constrain the attention weight allocation, helping learn better aspect-specific sentence representations. To the best of our knowledge, this is the first work for multi-aspect sentiment analysis.

%\vspace{-0.2cm}
%\item \textcolor{red}{For accurately constraining the sentences, we annotate two public datasets about whether multiple aspects in the sentence are overlap, and we will publish the datasets.} 

\vspace{-0.2cm}
\item We extend CAN to multi-task settings by introducing ACD as an auxiliary task, and applying CAN on  both ALSC and ACD tasks. 

\vspace{-0.2cm}
\item Extensive experiments are conducted on public datasets. Results demonstrate the effectiveness of our approach for aspect level sentiment classification.  
\end{itemize}

\section{Related Work}
%\subsection{Aspect Level Sentiment Classification}
{\bf Aspect level sentiment classification} is a fine-grained sentiment analysis task. Earlier methods are usually based on explicit features \cite{liu2010improving,Vo2015Target}. \cite{liu2010improving} uses different linguistic features for sentiment classification. \cite{Vo2015Target} studies aspect-based Twitter sentiment classification by applying automatic features, which are obtained from unsupervised learning methods. With the rapid development of deep learning technologies, many end-to-end neural networks are implemented to solve this fine-grained task. \cite{Wang2016Attention} proposes an attention-based LSTM network for aspect-level sentiment
classification. \cite{Tay2017Learning} introduces a word aspect fusion attention layer to learn attentive representations. \cite{Ma2017Interactive} proposes the interactive attention
networks to generate the representations for targets and contexts separately. \cite{tay2017dyadic} proposes dyadic memory networks for aspect based sentiment analysis. \cite{cheng2017aspect,ruder2016hierarchical} both propose hierarchical neural network models for aspect level sentiment classification. \cite{ma2018targeted} proposes a two-step attention model for targeted aspect-based sentiment analysis. \cite{wang2018learning} proposes a segmentation
attention based LSTM model for aspect level sentiment classification. However, all these works can be categorized as single-aspect sentiment analysis, which deals with aspects in a sentence separately, ignoring the orthogonality among multiple aspects. 

%\subsection{Multi-task Learning}
{\bf Multi-task learning} \cite{Caruana1997Multitask} solves multiple learning tasks at the same time, achieving improved performance by exploiting commonalities and differences across tasks. Multi-task learning has been used successfully in many  machine learning applications. \cite{Huang2018Multitask} learns both main task and auxiliary task jointly with shared representations, achieving improved performance in question answering. \cite{Toshniwal2017Multitask} uses low-level auxiliary tasks
for encoder-decoder based speech recognition, which suggests that the addition of auxiliary tasks can help in either optimization or generalization. \cite{yu2016learning} uses two auxiliary tasks to help induce a sentence embedding that works well across domains for sentiment classification. In this paper, we adopt the multi-task learning approach by using ACD as the auxiliary task to help the ALSC task. 

%In our implementation, the ACD task shares the same sentence representation and use the same attention mechanism with the ALSC task, so that we can easily extend our attention constraints to the ACD task. The multi-task learning can benefit from shared sentence representations, as well as attention constraints. 

\begin{figure*}
\setlength{\abovecaptionskip}{0.2cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.3cm}   %调整图片标题与下文距离
\centering
	\includegraphics[width=1.0\textwidth]{Big_network_new.pdf}
    \caption{Network Architecture. The aspect categories are embedded as vectors. The model encodes the sentence using LSTM. Based on its hidden states, aspect-specific sentence representations for ALSC and ACD tasks are learned via constrained attention. Then aspect level sentiment prediction and aspect category detection are made. }
    \label{network}
\end{figure*}
\section{Model}
We first formulate the problem. There are totally $N$ predefined aspect categories in the dataset, $A=\{A_1,...,A_N\}$. Given a sentence $S=\{w_1, w_2, ..., w_L\}$, which contains $K$ aspects $A^s=\{A_1^s,...,A_K^s\}, A^s\subseteq  A$, the multi-task learning is to simultaneously solve the ALSC and ACD tasks, namely, the ALSC task predicts the sentiment polarity of each aspect $A_k^s \in A^s$, and the auxiliary ACD task checks each aspect $A_n \in A$ to see whether the sentence $S$ mentions it.

We propose CAN for multi-aspect sentiment analysis, supporting both ALSC and ACD tasks by a multi-task learning framework. The network architecture is shown in Figure \ref{network}. We will introduce all components sequentially from left to right.

\subsection{Input and Embedding Layers}
Traditionally, aspect based sentiment analysis handles each aspect separately, one at a time. In such settings, a sentence $S$ with $K$ aspects will be copied to form $K$ instances, each of which is associated with a single aspect. For example, a sentence $S$ contains two aspects, $A_1^s$ with polarity $p_1$ , and $A_2^s$ with polarity $p_2$. Two instances, $\langle{S, A_1^s, p_1}\rangle$ and $\langle{S, A_2^s, p_2}\rangle$, will be constructed.

In this paper, our model is for multi-aspect sentiment analysis, handling multiple aspects of a sentence together. For the sentence $S$ with two aspects $A_1^s$ and $A_2^s$, the input to our model is $\langle{S, [A_1^s, A_2^s], [p_1, p_2]}\rangle$, as a single instance.

With embedding matrices, the input sentence $\{w_1, w_2, ..., w_L\}$ is converted to a sequence of vectors $\{v_1,v_2,...,v_L\}$, and the $K$ aspects of the sentence are transformed to vectors $\{u_1^s,...,u_K^s\}$, which is a subset of $\{u_1,...,u_N\}$, the vectors of all aspect categories. The embedding dimension is $d$.

\subsection{LSTM Layer}
The word embeddings of the sentence are then fed into an LSTM network \cite{Hochreiter1997Long}, which outputs hidden states $H=\{h_1,h_2,...,h_L\}$. At each time
step $l$, the hidden state $h_l$ of the LSTM is computed
by:
\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
   h_l = LSTM(h_{l-1},v_l)
\end{equation}
%where $v_l$ is the input vector at time step $l$ and $h_{l-1}$ is the hidden state of the LSTM in previous time step $l-1$. 
The size of the hidden state is also set to be $d$.

\subsection{Task-Specific Attention Layer}
Our multi-task learning framework supports both ALSC and ACD tasks. The two tasks share the hidden states from the LSTM layer, while compute their own attention weights separately. The attention weights are then used to compute aspect-specific sentence representations. 

{\bf ALSC Attention Layer}
The key idea of aspect level sentiment classification is to learn different attention weights for different aspects, so that different aspects can concentrate on different parts of the sentence. 
%Multi-aspect sentiment analysis simultaneously handles multiple aspects by adding constraints to their attention weights. 
We follow the approach in \cite{Bahdanau2015iclr} to compute the attention. Particularly, given the sentence $S$ with $K$ aspects, $A^s=\{A_1^s,...,A_K^s\}$, for each aspect $A_k^s$, its attention weights are calculated by:
\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
    \alpha_k = softmax({z^a}^\mathrm{T}tanh(W_{1}^{a}{H} + W_{2}^{a}(u_k^{s}\otimes{e_L}))) 
  \label{equation_absa_att}
\end{equation}
where $u_k^{s}$ is the embedding of the aspect $A_k^s$, $e_L\in\mathbb{R}^{L}$ is a
vector of $1$s, ${u_k^{s}}\otimes{e_L}$ is the operation repeatedly concatenating $u_k^{s}$ for $L$ times. $W_{1}^a\in\mathbb{R}^{{d}\times{d}}$, $W_{2}^a\in\mathbb{R}^{{d}\times{d}}$ and $z^a\in\mathbb{R}^{d}$ are the weight matrices.

{\bf ACD Attention Layer} We treat the ACD task as multi-label classification problem for the set of $N$ aspect categories. For each aspect $A_n\in A$, its attention weights are calculated by:
\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
    \beta_n = softmax({z^b}^\mathrm{T}tanh(W_{1}^{b}{H} + W_{2}^{b}(u_n\otimes{e_L}))) 
  \label{equation_acd_att}
\end{equation}
where $u_n$ is the embedding of the aspect $A_n$. $W_{1}^b\in\mathbb{R}^{{d}\times{d}}$, $W_{2}^b\in\mathbb{R}^{{d}\times{d}}$ and $z^b\in\mathbb{R}^{d}$ are the weight matrices.

The ALSC and ACD tasks use the same attention mechanism, but they do not share parameters. The reason to use separated parameters is that, for the same aspect, the attention of ALSC concentrates more on opinion words, while ACD focuses more on aspect target terms (see the attention visualizations in Section~\ref{sec:att_vis}). 
%Take Figure \ref{sentence} as an example, for the aspect \emph{food}, ALSC concentrates more on the word \emph{``great''}, while ACD puts more attention on the word \emph{``taste''}.

\subsection{Regularization Layer}
Multi-aspect sentiment analysis simultaneously handles multiple aspects by adding constraints to their attention weights. {\bf Note that this layer is only available in the training stage}, in which the ground-truth aspects are known for calculating the regularization loss, and then influence parameter updating in back propagation. While in the testing/inference stage, the true aspects are unknown and the regularization loss is not calculated so that this layer is omitted from the architecture.  

In this paper, we introduce two types of regularizations: the sparse regularization on each single aspect; the orthogonal regularization on multiple non-overlapping aspects. 

{\bf Sparse Regularization} For each aspect, the sparse regularization constrains the distribution of the attention weights ($\alpha_k$ or $\beta_n$) to concentrate on less words. For simplicity, we use $\alpha_k$ as an example, $\alpha_k=\{\alpha_{k1},  \alpha_{k2}, ..., \alpha_{kL}\}$. To make $\alpha_k$ sparse, the sparse regularization term is defined as: 
\begin{equation} 
\setlength{\abovedisplayskip}{4pt}  % 公式与上文间距
\setlength{\belowdisplayskip}{4pt}  % 公式与下文间距
    R_s = \mid\sum\limits_{l=1}^L{\alpha_{kl}^{2}} - 1\mid
    \label{equation:sparse_reg}
\end{equation}

where $\sum\limits_{l=1}^L{\alpha_{kl}}=1$ and $\alpha_{kl}>0$. Since $\alpha_k$ is normalized as a probability distribution, $L_1$ norm is always equal to $1$ (the sum of the probabilities) and does not work as sparse regularization as usual. Minimizing Equation \ref{equation:sparse_reg} will force the sparsity of $\alpha_k$. It has the similar effect as minimizing the entropy of $\alpha_k$, which leads to placing more probabilities on less words.

{\bf Orthogonal Regularization} This regularization term forces orthogonality between attention weight vectors of different aspects, so that different aspects attend on different parts of the sentence with less overlap. Note that we only apply this regularization to non-overlapping multi-aspect sentences. Assume that the sentence $S$ contains $K$ non-overlapping aspects $\{A_1^s,...,A_K^s\}$ and their attention weight vectors are $\{\alpha_1,...,\alpha_K\}$. We pack them together as a two-dimensional attention matrix $M\in\mathbb{R}^{{K}\times{L}}$ to calculate the orthogonal regularization term.

where $I$ is an identity matrix. In the resulted matrix of ${M^{\mathrm{T}}M}$, each non-diagonal element is the dot product between two attention weight vectors, minimizing the non-diagonal elements will force orthogonality between corresponding attention weight vectors. The diagonal elements of ${M^{\mathrm{T}}M}$ are subtracted by $1$, which are the same as $R_s$ defined in Equation \ref{equation:sparse_reg}. As a whole, $R_o$ includes both sparse and orthogonal regularization terms. 

Note that in the ACD task, we do not pack all the $N$ attention vectors $\{\beta_1, ..., \beta_N\}$ as a matrix. The sentence $S$ contains $K$ aspects. For simplicity, let $\{\beta_1, ..., \beta_K\}$ be the attention vectors of the $K$ aspects mentioned, while $\{\beta_{K+1}, ..., \beta_N\}$ be the attention vectors of the $N-K$ aspects not mentioned. We compute the average of the $N-K$ attention vectors, denoted by $\beta_{avg}$. We then construct the attention matrix $G=\{\beta_{1}, ..., \beta_{K},\beta_{avg}\}$, $G\in\mathbb{R}^{{(K+1)}\times{L}}$. The reason why we calculate $\beta_{avg}$ is that if an aspect is not mentioned in the sentence, its attention weights often attend to meaningless stop words, such as \emph{``to''}, \emph{``the''}, \emph{``was''}, etc. We do not need to distinguish among the $N-K$ aspects not mentioned, therefore they can share stop words in the sentence by being averaged as a whole, which keeps the $K$ aspects mentioned away from such stop words. 



\section{Experiments}
\subsection{Datasets}
We conduct experiments on two public datasets from SemEval 2014 task 4 \cite{Pontiki2014SemEval} and SemEval 2015 task 12 (denoted by Rest14 and Rest15 respectively). These two datasets consist of restaurant customer reviews with annotations identifying the mentioned aspects and the sentiment polarity of each aspect. To apply orthogonal regularization, we manually annotate the multi-aspect sentences with overlapping or non-overlapping. We randomly split the original training set into training, validation sets in the ratio 5:1, where the validation set is used to select the best model. We count the sentences of single-aspect and multi-aspect separately. Detailed statistics are summarized in Table \ref{table-dataset}. Particularly, $85.23\%$ and $83.73\%$ of the multi-aspect sentences are non-overlapping in Rest14 and Rest15, respectively.  

%%%%%%%Comment table
\begin{comment}
\begin{table}[t!]
\begin{center}
\setlength{\tabcolsep}{0.3mm}{
\begin{tabular} {|c|ccc|ccc|}
\hline
	\multirow{2}{*}{Dataset} &  \multicolumn{3}{c|}{\#sentences} & \multicolumn{3}{c|}{\#aspects}  \\
    \cline{2-7}
    & \emph{Single} &  \emph{Multi} &  Total &  \emph{Single} & \emph{Multi} & Total \\
	\hline
		Rest14\_Train & 2053 & 482 & 2535 & 2053 & 1047 & 3100\\
        Rest14\_Val & 412 & 94 & 506 & 412 & 201 & 613 \\
		Rest14\_Test & 611 & 189 & 800 & 611 & 414 & 1025 \\
        \hline
        Rest15\_Train & 622 & 309 & 931 & 622 & 766 & 1388 \\
        Rest15\_Val & 137 & 52 & 189 & 137 & 129 & 266 \\
        Rest15\_Test & 390 & 192 & 582 & 390 & 455 & 845 \\
	\hline
\end{tabular}}
\end{center}
\caption{\label{table-dataset} The numbers of single-aspect and multi-aspect sentences, and the numbers of aspects in single-aspect and multi-aspect sentences.}	
\end{table}
\end{comment}

\begin{table}[t!]
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离
\begin{center}
\setlength{\tabcolsep}{1mm}{
\begin{tabular} {|c|c|ccc|c|}
\hline
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{\#Single} & \multicolumn{3}{c|}{\#Multi} & \multirow{2}{*}{\#Total} \\
    \cline{3-5}
      &&  \emph{OL} & \emph{NOL} & \emph{Total} & \\
	\hline
		Rest14\_Train & 2053 & 67 & 415 & 482 & 2535\\
        Rest14\_Val & 412 & 19 & 75 & 94 & 506 \\
		Rest14\_Test & 611 & 27 & 162 & 189 & 800 \\
        \hline
        Rest15\_Train & 622 & 47 & 262 & 309 & 931 \\
        Rest15\_Val & 137 & 13 & 39 & 52 & 189 \\
        Rest15\_Test & 390 & 30 & 162 & 192 & 582 \\
	\hline
\end{tabular}}
\end{center}
\caption{\label{table-dataset} The numbers of single- and multi-aspect sentences. \emph{OL} and \emph{NOL} denote the overlapping and non-overlapping multi-aspect sentences, respectively.}	
\end{table}


\subsection{Comparison Methods}
\begin{itemize}
\item {\bf LSTM}: We implement the vanilla LSTM networks to model the sentence and use the average of all hidden states as the sentence representation. In this model, aspect information is not used.

\vspace{-6pt}
\item {\bf AT-LSTM} \cite{Wang2016Attention}: It adopts the attention mechanism in LSTM to generate a weighted representation of a sentence. The aspect embedding is used to compute the attention weights as in Equation \ref{equation_absa_att}. We do not concatenate the aspect embedding to the hidden state as in \cite{Wang2016Attention} and gain small performance improvement. We use this modified version of AT-LSTM in all experiments. 

\vspace{-6pt}
\item {\bf ATAE-LSTM} \cite{Wang2016Attention}: This method is an extension of AT-LSTM. In this model, the aspect embedding is concatenated to each word embedding of the sentence as the input to the LSTM layer. 

%\vspace{-5pt}
%\item {\bf AF-LSTM(CONV)} \cite{Tay2017Learning}: It utilizes circular convolution to compute deeper fusion relationships between each word in sentence and aspect.
\end{itemize}



\begin{table}[t!]
\setlength{\abovecaptionskip}{0.0cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离
\begin{center}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular} {|c|cc|cc|}
\hline
	\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Rest14} & \multicolumn{2}{c|}{Rest15} \\ 
    \cline{2-5}
    & 3-way & Binary & 3-way & Binary \\    
	\hline
    LSTM      & 80.61 & 86.66 & 73.14 & 73.27 \\ 
    AT-LSTM   & 81.66 & 87.13 & 75.15 & 76.40 \\
    ATAE-LSTM & 82.08 & 87.72 & 74.32 & 76.79 \\
    %AF-LSTM(CONV) & 81.29 & 87.26 & - & - \\
    \hline
    AT-CAN-$R_s$ & 81.97 & 88.08 & 75.74 & 80.05 \\
    AT-CAN-$R_o$ & 82.60 & 88.67 & 75.03 & 81.10 \\
    ATAE-CAN-$R_s$ & 82.29 & 87.37 &  76.09 & 80.83 \\
    ATAE-CAN-$R_o$ & {\bf 82.91} & {\bf 89.02} & {\bf 77.28} & {\bf 82.66} \\
	\hline
\end{tabular}}
\end{center}
\caption{\label{table-st} Results of the ALSC task in terms of accuracy ($\%$). All methods are run in single-task settings.}	
\end{table}

\subsection{Results}
Table \ref{table-st} and \ref{table-mt} show our experimental results on the two public datasets for single-task and multi-task settings respectively. In both tables, ``3-way'' stands for 3-class classification (positive, neutral, and negative), and ``Binary'' for binary classification (positive and negative). The best scores are marked in bold. 

\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.2cm}   %调整图片标题与下文距离
\centering
\subfigure[AT-LSTM]{
\includegraphics[width=0.45\textwidth]{at_.pdf}}
\vspace{-5pt}
\subfigure[M-AT-LSTM]{
\includegraphics[width=0.45\textwidth]{at_r2_.pdf}}
\vspace{-5pt}
\subfigure[M-CAN-2$R_o$]{
\includegraphics[width=0.45\textwidth]{multitask_at_2r2_.pdf}}
\caption{Visualization of attention weights of different aspects in the ALSC task. Three different models are compared.}
\label{compare-att}
\end{figure}

\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.4cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=0.45\textwidth]{f3_.pdf}
\caption{Visualization of attention weights of different aspects in the ACD task from M-CAN-2$R_o$. The a/m is short for anecdotes/miscellaneous.} 
  \label{ACD-att} 
\end{figure}

\begin{figure}
\setlength{\abovecaptionskip}{0.1cm}   %调整图片标题与图距离
\setlength{\belowcaptionskip}{-0.5cm}   %调整图片标题与下文距离
\centering
\includegraphics[width=0.45\textwidth]{loss.pdf}
\caption{The regularization loss curves of $R_s$ and $R_o$ during the training of AT-CAN-$R_o$.} 
  \label{figure:reg-loss} 
\end{figure}

{\bf Single-task Settings} Table \ref{table-st} shows our experimental results of aspect level sentiment classification in single-task settings. Firstly, we observe that by introducing attention regularizations (either $R_s$ or $R_o$), most of our proposed methods outperform their counterparts. Specifically, AT-CAN-$R_s$ and AT-CAN-$R_o$ outperform AT-LSTM in $7$ of $8$ results; ATAE-CAN-$R_s$ and ATAE-CAN-$R_o$ also outperform ATAE-LSTM in $7$ of $8$ results. For example, in the Rest15 dataset, ATAE-CAN-$R_o$ outperforms ATAE-LSTM by up to $7.64\%$ in the Binary classification. Secondly, regularization $R_o$ achieves better performance improvement than $R_s$ in all results. This is because $R_o$ includes both orthogonal and sparse regularizations for non-overlapping multi-aspect sentences. Finally, the LSTM method outputs the worst results in all cases, because it can not distinguish different aspects. We do not add regularization terms to the LSTM method since no attention weights are computed in this method. 

% 在Finally之前     It is worth noting that if a dataset contains only a small portion of multi-aspect sentences, the regularization $R_o$ may not outperform much than $R_s$.

\section{Conclusion}
We propose constrained attention networks for multi-aspect sentiment analysis, which handles multiple aspects of a sentence simultaneously. Specifically, we introduce orthogonal and sparse regularizations on attention weights. Furthermore, we introduce an auxiliary task ACD for promoting the ALSC task, and apply CAN on both tasks. Experimental results demonstrate that our approach outperforms the state-of-the-arts significantly. 

\bibliographystyle{acl_natbib}
\bibliography{naaclhlt2019}


\end{document}
